\section{Multiple Linear Regression}
Multiple regression: To quantify the relationship between several independent (predictor) variables and a dependent (response) variable. The coefficients ($a, b_{1} to b_{i}$) are estimated by the least squares method, which is equivalent to maximum likelihood estimation. A multiple regression model is built upon three major assumptions:

\begin{enumerate}
\item The response variable is normally distributed,
\item The residual variance does not vary for small and large fitted values (constant variance),
\item The observations (explanatory variables) are independent.
\end{enumerate}





\section{Multiple Linear Regression}
\subsection{What is Multiple Linear Regression}

Multiple regression is a statistical technique that allows us to predict a numeric value on the response variable on the basis of the observed values on several other independent variables.

Suppose we were interested in predicting how much an individual enjoys their job. Variables such as salary, extent of academic qualifications, age, sex, number of years in full-time employment and socioeconomic status might all contribute towards job satisfaction. If we collected data on all of these variables, perhaps by surveying a few hundred members of the public, we would be able to see how many and which of these variables gave rise to the most accurate prediction of job satisfaction. We might find that job satisfaction is most accurately predicted by type of occupation, salary and years in full-time employment, with the other variables not helping us to predict job satisfaction.

\[\hat{y} = b_0 + b_1x_1 + b_2x_2 + \ldots \]

\begin{itemize}
\item $\hat{y}$ is the \textbf{\textit{fitted value}} for the dependent variable \textbf{$Y$}, given a linear combination of values for the independent valriables.

\item $x_1$ is the value for independent variable \textbf{$X_1$}.
\item $b_o$ is the constant regression estimate ( commonly known as the \textbf{Intercept Estimate} in the case of simple linear regression).
\end{itemize}
\newpage
\subsection*{Multiple Linear Regression}

%Previously we have seem SLR the case of one dependent variable Y explained by one independent variable X. 

Multiple regression analysis is an extension of simple regression analysis, as described previously, to applications involving the use of two or more \textbf{\textit{independent variables}} (predictors) to estimate the value of the \textbf{\textit{dependent variable}} (response variable).
In the case of two independent variables, denoted by X1 and X2, the linear algebraic model is
\[ \hat{Y_i} = \beta_0 + \beta_1X_{i,1} + \beta_2X_{i,2} +\varepsilon_I\]

The definitions of the above terms are equivalent to the definitions in previous classes for simple regression analysis, except that more than one independent variable is involved in the present case.

Based on sample data, the linear regression equation for the case of two independent variables is
\[ \hat{Y} = b_0 + b_1X_1 + b_2X_2 \]

The multiple regression equation identifies the best-fitting line based on the method of least squares. In the case of multiple regression analysis, the best-fitting line is a line through
n-dimensional space (3-dimensional in the case of two independent variables).


\section{Multiple Linear Regression}
\subsection{What is Multiple Linear Regression}

Multiple regression is a statistical technique that allows us to predict a numeric value on the response variable on the basis of the observed values on several other independent variables.

Suppose we were interested in predicting how much an individual enjoys their job. Variables such as salary, extent of academic qualifications, age, sex, number of years in full-time employment and socioeconomic status might all contribute towards job satisfaction. If we collected data on all of these variables, perhaps by surveying a few hundred members of the public, we would be able to see how many and which of these variables gave rise to the most accurate prediction of job satisfaction. We might find that job satisfaction is most accurately predicted by type of occupation, salary and years in full-time employment, with the other variables not helping us to predict job satisfaction.

\[\hat{y} = b_0 + b_1x_1 + b_2x_2 + \ldots \]

\begin{itemize}
	\item $\hat{y}$ is the \textbf{\textit{fitted value}} for the dependent variable \textbf{$Y$}, given a linear combination of values for the independent valriables.
	
	\item $x_1$ is the value for independent variable \textbf{$X_1$}.
	\item $b_o$ is the constant regression estimate ( commonly known as the \textbf{Intercept Estimate} in the case of simple linear regression).
\end{itemize}
\newpage


\section{Terminology}
\subsection{Beta (standardised regression coefficients)}
The beta value is a measure of how strongly each predictor variable influences the
response variable. The beta is measured in units of standard deviation. For example,
a beta value of 2.5 indicates that a change of one standard deviation in the predictor
variable will result in a change of 2.5 standard deviations in the response variable.
Thus, the higher the beta value the greater the impact of the predictor variable on
the response variable.


The Standardized Beta
Coefficients give a measure of the
contribution of each variable to
the model. A large value indicates
that a unit change in this
predictor variable has a large
effect on the criterion variable.
The t and Sig (p) values give a
rough indication of the impact of
each predictor variable â€“ a big
absolute t value and small p value
suggests that a predictor variable
is having a large impact on the
criterion variable.

\end{document}