
%======================================================%
\begin{frame}
	\frametitle{Machine Learning} 
	\large 

Question 1

Suppose that you have trained a logistic regression classifier, and it outputs on a new examplex a prediction h\theta(x) = 0.7.
This means (check all that apply): 
 Our estimate for$ \Pr(y|=1|x;\theta)$ is 0.7.
 Our estimate for$ \Pr(y|=0|x;\theta)$ is 0.3.
 Our estimate for$ \Pr(y|=1|x;\theta)$ is 0.3.
 Our estimate for$ \Pr(y|=0|x;\theta)$ is 0.7.
\end{frame}
%======================================================%
\begin{frame}
	\frametitle{Machine Learning} 
	\large 
Solution
 
Our estimate for $ \Pr(y|=1|x;\theta)$ is 0.7.  T h\theta(x)is precisely$ \Pr(y|=1|x;\theta)$ , so each is 0.7.
Our estimate for $ \Pr(y|=0|x;\theta)$ is 0.3.  T Since we must have $ \Pr(y|=0|x;\theta)$ = 1−$ \Pr(y|=1|x;\theta)$ , the former is 1−0.7=0.3 .
Our estimate for $ \Pr(y|=1|x;\theta)$ is 0.3.  F h\theta(x) gives $ \Pr(y|=1|x;\theta)$ , not 1−$ \Pr(y|=1|x;\theta)$ . 
Our estimate for $ \Pr(y|=0|x;\theta)$ is 0.7.  F h\theta(x) is $ \Pr(y|=1|x;\theta)$ , not $ \Pr(y|=0|x;\theta)$ 
\end{frame}
%======================================================%
\begin{frame}
	\frametitle{Machine Learning} 
	\large 

Question 3
 
Suppose you have the following training set, and fit a logistic regression classifier h\theta(x)=g(\theta0+\theta1x1+\theta2x2) .
  
Which of the following are true? Check all that apply. 
 
\end{frame}
%======================================================%
\begin{frame}
	\frametitle{Machine Learning} 
	\large 
 Adding polynomial features (e.g., instead using h(x)=g(0+1x1+2x2+3x12+4x1x2+5x22) ) could increase how well we can fit the training data.  
The positive and negative examples cannot be separated using a straight line. So, gradient descent will fail to converge.
At the optimal value of\theta (e.g., found by fminunc), we will have $ J(\theta) \geq $0 . 
 Because the positive and negative examples cannot be separated using a straight line, linear regression will perform as well as logistic regression on this data.
solution
 
1)
Adding polynomial features (e.g., instead using h(x)=g(0+1x1+2x2+3x12+4x1x2+5x22) ) could increase how well we can fit the training data.  
TRUE
Adding new features can only improve the fit on the training set: since setting\theta3=\theta4=\theta5=0 makes the hypothesis the same as the original one, gradient descent will use those features (by making the corresponding\thetaj non-zero) only if doing so improves the training set fit. 
 
2)
The positive and negative examples cannot be separated using a straight line. So, gradient descent will fail to converge. 
FALSE
While it is true they cannot be separated, gradient descent will still converge to the optimal fit. Some examples will remain misclassified at the optimum. 
\end{frame}
%======================================================%
\begin{frame}
	\frametitle{Machine Learning} 
	\large 
3)
At the optimal value of\theta (e.g., found by fminunc), we will have $ J(\theta)$$≥$0 . 
TRUE
The cost function $ J(\theta)$ is always non-negative for logistic regression. 
 
4)
Because the positive and negative examples cannot be separated using a straight line, linear regression will perform as well as logistic regression on this data. 
FALSE
While it is true they cannot be separated, logistic regression will outperform linear regression since its cost function focuses on classification, not prediction. 
\end{frame}
%======================================================%
\begin{frame}
\frametitle{Machine Learning} 
\large 

Question 5
Which of the following statements are true? Check all that apply.
 For logistic regression, sometimes gradient descent will converge to a local minimum (and fail to find the global minimum). This is the reason we prefer more advanced optimization algorithms such as fminunc (conjugate gradient/BFGS/L-BFGS/etc).
 Since we train one classifier when there are two classes, we train two classifiers when there are three classes (and we do one-vs-all classification).
 The sigmoid function g(z) is never greater than one (>1). g(z)=11+e-z
 The one-vs-all technique allows you to use logistic regression for problems in which eachy(i) comes from a fixed, discrete set of values.
\end{frame}
%======================================================%
\begin{frame}
	\frametitle{Machine Learning} 
	\large 

Solutions
1)
For logistic regression, sometimes gradient descent will converge to a local minimum (and fail to find the global minimum). This is the reason we prefer more advanced optimization algorithms such as fminunc (conjugate gradient/BFGS/L-BFGS/etc).  0.00
The cost function for logistic regression is convex, so gradient descent will always converge to the global minimum. We still might use a more advanded optimization algorithm since they can be faster and don't require you to select a learning rate. 
2)
Since we train one classifier when there are two classes, we train two classifiers when there are three classes (and we do one-vs-all classification).  
We need to train three classifiers if there are three classes; each one treats one of the three classes as the y=1 examples and the rest as the y=0 examples. 
3)
The sigmoid function g(z) is never greater than one (>1). g(z)=11+e-z  
 
The denomiator ranges from  to1 asz grows, so the result is always in(0,1) . 
4)
The one-vs-all technique allows you to use logistic regression for problems in which each y(i) comes from a fixed, discrete set of values. 
If each y(i) is one of k different values, we can give a label to each y(i)∈{1,2,…,k} and use one-vs-all as described in the lecture. 
\end{frame}

