\documentclass[a4paper,12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{eurosym}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{framed}
\usepackage{subfigure}
\usepackage{fancyhdr}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2570}
%TCIDATA{<META NAME="SaveForMode"CONTENT="1">}
%TCIDATA{LastRevised=Wednesday, February 23, 201113:24:34}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{Language=American English}

\pagestyle{fancy}
\setmarginsrb{20mm}{0mm}{20mm}{25mm}{12mm}{11mm}{0mm}{11mm}
\lhead{MA4128} \rhead{Kevin O'Brien} \chead{Week 8 Part B} %\input{tcilatex}

%http://www.electronics.dit.ie/staff/ysemenova/Opto2/CO_IntroLab.pdf
\begin{document}

\section{Akaike's information criterion}
\begin{itemize}
    \item Akaike's information criterion is a measure of the goodness of fit of an estimated statistical model. 
    The AIC was developed by Hirotsugu Akaike under the name of "an information criterion" in 1971.

\item The AIC is a "model selection" tool i.e. a method of comparing two or more candidate models.

\item The AIC is calculated using the "likelihood function" and the number of parameters. (Not on course). The likelihood value is generally given in code output, as a complement to the AIC.

\item 
The AIC methodology attempts to find the model that best explains the data with a minimum of parameters. (i.e. in keeping with the Law of parsimony)

\item 
Given a data set, several competing models may be ranked according to their AIC, with the one having the lowest AIC being the best.

\item 
(A difference in AIC values of less than two is considered negligible)
\end{itemize}




\bigskip
%AIC provides a means for comparison among modelsa tool for model selection.
%\bigskip
%AIC is good for prediction.\\

\[\mbox{AIC} = 2p - 2\ln(L)\]

\begin{itemize}
	\item $p$ is the number of free model parameters.
	\item $L$ is the value of the Likelihood function for the model in question.
	\item For AIC to be optimal, $n$ must be large compared to $p$.\\
\end{itemize}
\subsubsection{Schwarz's Bayesian Information Criterion}
An alternative to the AIC is the Schwarz BIC, which additionally takes into account the sample size $n$.

\[\mbox{BIC} = p\ln{n} - 2\ln(L)\]



\section{Information Criterions}


We define two types of information criterion: the Bayesian Information
Criterion (BIC) and the Akaike Information Criterion (AIC). In AIC and BIC, we choose the model that
has the minimum value of:
\[AIC = −2log(L)+2m,\]
\[BIC = −2log(L)+mlogn\]

where
\begin{itemize}
\item L is the likelihood of the data with a certain model,
\item n is the number of observations and
\item m is the number of parameters in the model.
\end{itemize}
\subsection{AIC}
The Akaike information criterion is a measure of the relative \textbf{goodness of fit} of a statistical model.

When using the AIC for selecting the parametric model class, choose
the model for which the AIC value is lowest.


\end{document}
