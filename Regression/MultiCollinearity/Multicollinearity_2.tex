\documentclass[a4paper,12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{fancyhdr}
%\usepackage{listings}
\usepackage{framed}
\usepackage{graphicx}

\setcounter{MaxMatrixCols}{10}


\pagestyle{fancy}
\setmarginsrb{20mm}{0mm}{20mm}{25mm}{12mm}{11mm}{0mm}{11mm}
\lhead{MA4128} \rhead{Mr. Kevin O'Brien}
\chead{Advanced Data Modelling}
%\input{tcilatex}


% http://www.norusis.com/pdf/SPC_v13.pdf
\begin{document}
	
	\section{Overfitting}
	Overfitting occurs when a statistical model does not adequately describe of the underlying relationship between variables in a regression model. Overfitting generally occurs when the model is excessively complex, such as having too many parameters (i.e. predictor variables) relative to the number of observations. A model which has been overfit will generally have poor predictive performance, as it can exaggerate minor fluctuations in the data.
	
\begin{itemize}
	\item \textbf{Multi-collinearity}: Multi-collinearity occurs when two or more predictors in the model are
	correlated and provide redundant information about the response. Examples of pairs of multi-collinear predictors are years of education and income, height and weight of a person, and assessed value and square footage
	of a house.
	
	\item \textbf{Consequences of high multicollinearity}:
	Multi-collinearity leads to decreased reliability and predictive power of statistical models, and hence, very often, confusing and misleading results.
%	\item Multicollinearity will be dealt with in a future component of this course: Variable Selection Procedures.
\end{itemize}
\section{Consequences of Multicollinearity}

In statistics, the occurrence of several independent variables in a multiple regression model are closely correlated to one another. 
Multicollinearity can cause strange results when attempting to study how well individual independent variables contribute to an understanding of the dependent variable. 
In general, multicollinearity can cause wide confidence intervals and strange $p-$values for independent variables.




\section{Multicollinearity}
\begin{itemize}
	\item In multiple regression, two or more predictor variables are colinear if they show strong linear relationships. This makes estimation of regression coefficients impossible. It can also produce unexpectedly large estimated standard errors for the coefficients of the X variables involved.
	\item This is why an exploratory analysis of the data should be first done to see if any collinearity among explanatory variables exists.
	
	
	\item Multicolinearity is suggested by non-significant results in individual tests on the regression coefficients for important explanatory (predictor) variables. \item Multicolinearity may make the determination of the main predictor variable having an effect on the outcome difficult.
\end{itemize}
%============================================================================%


\begin{itemize}
	\item When choosing a predictor variable you should select one that might be correlated with the criterion variable, but that is not strongly correlated with the other predictor variables.
	\item However, correlations amongst the predictor variables are not unusual. The term multi-collinearity  is used to describe the situation
	when a high correlation is detected between two or more predictor variables.
	\item Such high correlations cause problems when trying to draw inferences about the relative contribution of each predictor variable to the success of the model.
\end{itemize} 



	
	\subsection{Types of multicollinearity}
	%http://online.stat.psu.edu/online/development/stat501/12multicollinearity/02multico_whatis.html
	
	%------------------------------------------------------------------------------------------------------%
	
	There are two types of multicollinearity: 
	\begin{itemize}
		\item Structural multicollinearity
		\item Data-based multicollinearity
	\end{itemize}
	Structural multicollinearity is a mathematical artifact caused by creating new predictors from other predictors — such as, creating the predictor x2 from the predictor x. 
	Data-based multicollinearity, on the other hand, is a result of a poorly designed experiment, reliance on purely observational data, or the inability to manipulate the system on which the data are collected. 
	In the case of structural multicollinearity, the multicollinearity is induced by what you have done. Data-based multicollinearity is the more troublesome of the two types of multicollinearity. Unfortunately it is the type we encounter most often!
	
	%------------------------------------------------------------------------------------------------------%
	


\section{How to Identify Multicollinearity}

\begin{itemize}
	\item You can assess multicollinearity by examining two collinearity diagnostic  measures: \textbf{tolerance} and the \textbf{Variance Inflation Factor} (VIF) .
	\item Tolerance is a measure of collinearity reported by most statistical programs such as SPSS; the variables tolerance is $1-R^2$. 

	\item All variables involved in the linear relationship will have a small tolerance. 
	\item \textbf{Interpretation:} A small tolerance value indicates that the variable under consideration is almost a perfect linear combination of the independent variables already in the equation and that it should not be added to the regression equation. 
	\item \textbf{Interpretation:} Some suggest that a tolerance value less than 0.1 should be investigated further. If a low tolerance value is accompanied by large standard errors and nonsignificance, multicollinearity may be an issue.
		
		\item The variance inflation factor (VIF) quantifies the severity of multicollinearity in a regression analysis.
		
		\item The VIF provides an index that measures how much the variance (the square of the estimate's standard deviation) of an estimated regression coefficient is increased because of collinearity.
	\item	A common rule of thumb is that if the VIF is greater than 5 then multicollinearity is high. Also a VIF level of 10 has been proposed as a cut off value.	
\end{itemize}	
%	
		
\subsection{Multicollinearity}
When choosing a predictor variable you should select one that might be correlated with the criterion variable, but that is not strongly correlated with the other predictor variables. However, correlations amongst the predictor variables are not unusual. The term multicollinearity (or collinearity) is used to describe the situation
when a high correlation is detected between two or more predictor variables.
%
Such high correlations cause problems when trying to draw inferences about the relative contribution of each predictor variable to the success of the model. SPSS provides you with a means of checking for this and we describe this below.

\subsection{Variance Inflation Factor (VIF)}
%
The variance inflation factor (or “VIF”) provides us with a measure of how much the variance for a given regression coefficient is increased compared to if all predictors were uncorrelated. To understand what the variance inflation factor is, and what it measures, we need to examine the computation of the standard error of a regression coefficient.
%
\section{Tolerance}
%
Tolerance is simply the reciprocal of VIF, and is computed as
\[ \mbox{Tolerance} = \frac{1}{VIF}\]
Whereas large values of VIF were unwanted and undesirable, since tolerance is the reciprocal of VIF, larger than not values of tolerance are indicative of a lesser problem with collinearity. In other words, we want large tolerances.


%-------------------------------------------------------------- %
\subsection{The Variance Inflation Factor (VIF)}


\begin{itemize}
	\item The Variance Inflation Factor (VIF) measures the impact of collinearity among the variables in a regression model. 
	\item The Variance Inflation Factor (VIF) is 1/Tolerance, it is always greater than or equal to 1. 
	\item There is no formal VIF value for determining presence of multicollinearity. Values of VIF that exceed 10 are often regarded as indicating multicollinearity, but in weaker models values above 2.5 may be a cause for concern. 
	\item In many statistics programs, the results are shown both as an individual $R^2$ value (distinct from the overall R2 of the model) and a Variance Inflation Factor (VIF). 
	\item When those $R^2$ and VIF values are high for any of the variables in your model, multicollinearity is probably an issue. 
	\item When VIF is high there is high multicollinearity and instability of the b and beta coefficients. It is often difficult to sort this out. 
\end{itemize}


\bigskip

You can also assess multicollinearity in regression in the following ways:

\begin{itemize}
	\item [(1)] Examine the correlations and associations (nominal variables) between independent variables to detect a high level of association. High bivariate correlations are easy to spot by running correlations among your variables. If high bivariate correlations are present, you can delete one of the two variables. However, this may not always be sufficient.
	
	\item [(2)] Regression coefficients will change dramatically according to whether other variables are included or excluded from the model. Play around with this by adding and then removing variables from your regression model.
	
	\item [(3)] The standard errors of the regression coefficients will be large if multicollinearity is an issue.
	
	\item [(4)] Predictor variables with known, strong relationships to the outcome variable will not achieve statistical significance. In this case, neither may contribute significantly to the model after the other one is included. But together they contribute a lot. If you remove both variables from the model, the fit would be much worse. So the overall model fits the data well, but neither X variable makes a significant contribution when it is added to your model last. When this happens, multicollinearity may be present.
	\item Variance inflation factor and tolerance tolerance. One is the reciprocal of the other. 
	
\end{itemize}
%====================================================%












		



\subsection{Determing the Variance Inflation Factor (VIF) with \texttt{R}}


\begin{framed}
	\begin{verbatim}
	library(car)
	# Evaluate Collinearity
	vif(fit) # variance inflation factors 
	sqrt(vif(fit)) > 2 # problem?
	\end{verbatim}
\end{framed}





	\subsection{Interpreting Variance Inflation Factors}
	
	% http://online.stat.psu.edu/online/development/stat501/12multicollinearity/05multico_vif.html
	
\begin{itemize}
\item 	We learned previously that the standard errors, and hence the variances, of 
	the estimated coefficients are inflated when multicollinearity exists. 
\item 	So, the variance inflation factor for the estimated coefficient $b_k$, denoted $VIF_k$, 
	is just the factor by which the variance is inflated. 
	
\item 	Variance inflation factors k greater than 4 suggest that the multicollinearity should be investigated. 
\item 	Variance inflation factors greater than 10 are taken as an indication that the multicollinearity may be unduly influencing the least squares estimates. 
\end{itemize}	

	
	%------------------------------------------------------------------------------------------------------%
	


\subsection{Tolerance}
\begin{itemize}
\item Tolerance is simply the reciprocal of VIF, and is computed as
\[ \mbox{Tolerance} = \frac{1}{VIF}\]
\item Whereas large values of VIF were unwanted and undesirable, since tolerance is the reciprocal of VIF, larger than not values of tolerance are indicative of a lesser problem with collinearity. In other words, we want large tolerances.
\item 	A tolerance close to 1 means there is little multicollinearity, whereas a value close to 0 suggests that multicollinearity may be a threat. 


\item The VIF shows us how much the variance of the coefficient estimate is being inflated by multicollinearity. For example, if the VIF for a variable were 9, its standard error would be three times as large as it would be if its VIF was 1. In such a case, the coefficient would have to be 3 times as large to be statistically significant.


\end{itemize}




\end{document}
